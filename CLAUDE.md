# CLAUDE.md

This file provides guidance to Claude Code (claude.ai/code) when working with code in this repository.

## Project Overview

NetTap is an open-source, hardware-agnostic network visibility appliance that sits transparently inline between an ISP modem and a home/small business router. It provides enterprise-grade network telemetry via a polished web dashboard without requiring deep networking knowledge.

The project wraps CISA's Malcolm stack (Zeek, Suricata, Arkime, OpenSearch) with simplified deployment, a consumer-friendly setup wizard, and intelligent storage management.

**PRD:** `NetTap_PRD_v1.0.md` contains the full product specification.

## Project Structure

```
scripts/           Shell scripts for system setup
  bridge/          Linux bridge configuration (setup-bridge.sh)
  install/         Installation automation (install.sh)
  common.sh        Shared shell utilities (logging, root check, env loading)
daemon/            Python storage & health daemon
  storage/         Rolling retention manager (OpenSearch ILM + disk monitoring)
  smart/           SSD SMART health monitoring
  main.py          Daemon entry point (periodic storage + health checks)
web/               Web UI (framework TBD)
  wizard/          First-run setup wizard
  dashboard/       Main traffic dashboard
config/            Configuration files mounted into containers
  malcolm/         Malcolm stack overrides
  opensearch/      ILM policies (ilm-policy.json)
  suricata/        Suricata config overrides (nettap.yaml)
  zeek/            Zeek script overrides (nettap.zeek)
  grafana/         Grafana dashboard JSON + provisioning
docker/            Docker Compose and Dockerfiles
  docker-compose.yml   NetTap services (extends Malcolm)
  Dockerfile.daemon    Python daemon container
  Dockerfile.web       Web UI container
```

## Commands

```bash
# Install (on target Ubuntu host, as root)
sudo scripts/install/install.sh

# Bridge setup only
sudo scripts/bridge/setup-bridge.sh --wan eth0 --lan eth1

# Start NetTap services
docker compose -f docker/docker-compose.yml up -d

# Run daemon locally (development)
cd daemon && python3 main.py
```

## Technology Stack

- **Host OS:** Ubuntu Server 22.04 LTS
- **Orchestration:** Docker + Docker Compose
- **Network Analysis:** Zeek 6.x (metadata), Suricata 7.x (IDS), Arkime 5.x (PCAP)
- **Data Store:** OpenSearch 2.x + OpenSearch Dashboards
- **Visualization:** Grafana 10.x (optional enhanced dashboards)
- **Storage Daemon:** Python (custom rolling retention + SMART monitoring)
- **Log Forwarding:** Logstash (optional SIEM export)

## Architecture

### Network Layer
- Linux software bridge (`br0`) binding two physical NICs — transparent Layer 2 forwarding
- WAN NIC (`eth0`) connects to ISP modem, LAN NIC (`eth1`) connects to router
- Separate management interface (3rd NIC, Wi-Fi, or VLAN) for dashboard access
- Passive capture in promiscuous mode — appliance is invisible on the data path

### Data Pipeline
1. Raw packets captured on bridge interface
2. Zeek generates structured metadata logs (conn, DNS, HTTP, TLS, files, DHCP, SMTP)
3. Suricata performs signature/anomaly-based IDS with Emerging Threats ruleset
4. Logs enriched with GeoIP, ASN, hostname resolution
5. OpenSearch indexes all logs and alerts
6. Dashboards (OpenSearch Dashboards + Grafana) visualize the data

### Storage (Three-Tier)
| Tier | Data | Retention | Compression | Daily Size |
|------|------|-----------|-------------|------------|
| Hot | Zeek metadata logs | 90 days | zstd ~8:1 | 300-800 MB |
| Warm | Suricata alerts | 180 days | zstd ~6:1 | 10-50 MB |
| Cold | Raw PCAP (alert-triggered only) | 30 days | zstd ~3:1 | Variable |

OpenSearch ILM handles hot-tier rotation. A custom Python daemon monitors disk utilization with an 80% threshold safeguard.

## Development Phases

1. **Core Infrastructure** — Linux bridge scripts, Malcolm deployment automation, hardware validation
2. **Storage Management** — ILM config, retention daemon, SMART monitoring, compression
3. **Onboarding UX** — Setup wizard, NIC auto-detection, admin UI
4. **Dashboard Polish** — Custom Grafana dashboards, GeoIP maps, bandwidth trending, notifications
5. **Community Release** — Docs, install script, community setup

## Hooks

This project has two Claude Code hooks configured in `.claude/settings.json`:

### PreToolUse: File Protection (`.claude/hooks/protect-files.sh`)
A command hook on `Edit|Write` that blocks edits to protected files. Exit code 2 blocks the tool call; stderr is shown to Claude as feedback. Protected files:
- `.env` / `.env.*` — environment secrets, must be edited manually
- `package-lock.json` — auto-generated by `npm install`
- `yarn.lock` — auto-generated by `yarn install`
- `.git/*` — must use git commands

The script reads `tool_input.file_path` from stdin JSON using `python3` (not `jq`, which may not be available).

### Stop: Test Coverage Gatekeeper
An agent hook that fires when Claude tries to stop. It reads the session transcript to find all `.ts` files modified via Edit/Write, then uses Glob to verify each has a matching `.test.ts` or `.spec.ts` file. If tests are missing, it returns `{"ok": false, "reason": "..."}` to block the stop and instruct Claude to create them. Checks `stop_hook_active` to avoid infinite loops.

## Key Design Constraints

- Target hardware: Intel N100 mini PCs, 16GB RAM, 1TB NVMe, dual Intel i226-V 2.5GbE NICs (~$200 BOM)
- Must add <1ms latency to traffic path
- Zero packet loss at 500Mbps sustained, support up to 1Gbps line rate
- Dashboard loads <3s on LAN; alerts surface <10s from detection
- Pre-configure OpenSearch JVM heap caps to prevent OOM on 16GB systems
- Pin Malcolm to tested release tags to avoid upstream breaking changes
- SSD write endurance: implement write coalescing, 30s log flush intervals, SMART monitoring
- No cloud dependency, no telemetry, no subscription — fully self-contained
- v1.0 is read-only visibility (no blocking/firewall), no TLS payload decryption

## Licensing

All core components use permissive open-source licenses (Apache 2.0, BSD, GPLv2). Grafana is AGPLv3. The project itself should use a compatible open-source license.
